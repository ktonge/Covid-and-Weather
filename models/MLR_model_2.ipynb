{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sqlalchemy import create_engine\n",
    "from config import DB_NAME, DB_URL, DB_PORT, DB_NAME, USERNAME, PASSWORD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_string = \"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}\".format(\n",
    "    user = USERNAME,\n",
    "    password = PASSWORD,\n",
    "    host = DB_URL,\n",
    "    port = DB_PORT,\n",
    "    database = DB_NAME\n",
    ")\n",
    "\n",
    "engine = create_engine(engine_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(psycopg2.OperationalError) connection to server at \"finalgroup2.cjwyb8n4feqd.us-east-2.rds.amazonaws.com\" (18.189.90.220), port 5432 failed: FATAL:  database \"covid_and_weather_counties\" does not exist\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/engine/base.py:145\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbapi_connection \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mraw_connection()\n\u001b[1;32m    146\u001b[0m \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mloaded_dbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/engine/base.py:3275\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3254\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[1;32m   3255\u001b[0m \n\u001b[1;32m   3256\u001b[0m \u001b[39mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3273\u001b[0m \n\u001b[1;32m   3274\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3275\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool\u001b[39m.\u001b[39;49mconnect()\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/base.py:455\u001b[0m, in \u001b[0;36mPool.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \n\u001b[1;32m    450\u001b[0m \u001b[39mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m \n\u001b[1;32m    454\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[39mreturn\u001b[39;00m _ConnectionFairy\u001b[39m.\u001b[39;49m_checkout(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/base.py:1271\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[0;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fairy:\n\u001b[0;32m-> 1271\u001b[0m     fairy \u001b[39m=\u001b[39m _ConnectionRecord\u001b[39m.\u001b[39;49mcheckout(pool)\n\u001b[1;32m   1273\u001b[0m     \u001b[39mif\u001b[39;00m threadconns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/base.py:719\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[0;34m(cls, pool)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 719\u001b[0m     rec \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_do_get()\n\u001b[1;32m    721\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/impl.py:168\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[1;32m    169\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dec_overflow()\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:147\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    148\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/impl.py:166\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection()\n\u001b[1;32m    167\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/base.py:396\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m \u001b[39mreturn\u001b[39;00m _ConnectionRecord(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/base.py:681\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[0;34m(self, pool, connect)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[39mif\u001b[39;00m connect:\n\u001b[0;32m--> 681\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__connect()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalize_callback \u001b[39m=\u001b[39m deque()\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/base.py:905\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[1;32m    906\u001b[0m         pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mError on connect(): \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, e)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:147\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    148\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/base.py:901\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarttime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbapi_connection \u001b[39m=\u001b[39m connection \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_invoke_creator(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    902\u001b[0m pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreated new connection \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, connection)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/engine/create.py:636\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[0;34m(connection_record)\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[39mreturn\u001b[39;00m connection\n\u001b[0;32m--> 636\u001b[0m \u001b[39mreturn\u001b[39;00m dialect\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/engine/default.py:580\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[0;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams):\n\u001b[1;32m    579\u001b[0m     \u001b[39m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloaded_dbapi\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/psycopg2/__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 122\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39;49mconnection_factory, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwasync)\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mOperationalError\u001b[0m: connection to server at \"finalgroup2.cjwyb8n4feqd.us-east-2.rds.amazonaws.com\" (18.189.90.220), port 5432 failed: FATAL:  database \"covid_and_weather_counties\" does not exist\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_sql_table(\u001b[39m'\u001b[39;49m\u001b[39mtablename\u001b[39;49m\u001b[39m'\u001b[39;49m,engine)\n\u001b[1;32m      2\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/pandas/io/sql.py:281\u001b[0m, in \u001b[0;36mread_sql_table\u001b[0;34m(table_name, con, schema, index_col, coerce_float, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[39mRead SQL database table into a DataFrame.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39m>>> pd.read_sql_table('table_name', 'postgres:///db_name')  # doctest:+SKIP\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    280\u001b[0m pandas_sql \u001b[39m=\u001b[39m pandasSQL_builder(con, schema\u001b[39m=\u001b[39mschema)\n\u001b[0;32m--> 281\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pandas_sql\u001b[39m.\u001b[39;49mhas_table(table_name):\n\u001b[1;32m    282\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTable \u001b[39m\u001b[39m{\u001b[39;00mtable_name\u001b[39m}\u001b[39;00m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    284\u001b[0m \u001b[39m# error: Item \"SQLiteDatabase\" of \"Union[SQLDatabase, SQLiteDatabase]\"\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[39m# has no attribute \"read_table\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/pandas/io/sql.py:1760\u001b[0m, in \u001b[0;36mSQLDatabase.has_table\u001b[0;34m(self, name, schema)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhas_table\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m, schema: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1758\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39msqlalchemy\u001b[39;00m \u001b[39mimport\u001b[39;00m inspect \u001b[39mas\u001b[39;00m sqlalchemy_inspect\n\u001b[0;32m-> 1760\u001b[0m     insp \u001b[39m=\u001b[39m sqlalchemy_inspect(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnectable)\n\u001b[1;32m   1761\u001b[0m     \u001b[39mreturn\u001b[39;00m insp\u001b[39m.\u001b[39mhas_table(name, schema \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta\u001b[39m.\u001b[39mschema)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/inspection.py:111\u001b[0m, in \u001b[0;36minspect\u001b[0;34m(subject, raiseerr)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39melif\u001b[39;00m reg \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m subject\n\u001b[0;32m--> 111\u001b[0m ret \u001b[39m=\u001b[39m reg(subject)\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/engine/reflection.py:304\u001b[0m, in \u001b[0;36mInspector._engine_insp\u001b[0;34m(bind)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39m@inspection\u001b[39m\u001b[39m.\u001b[39m_inspects(Engine)\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_engine_insp\u001b[39m(bind: Engine) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Inspector:  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m     \u001b[39mreturn\u001b[39;00m Inspector\u001b[39m.\u001b[39;49m_construct(Inspector\u001b[39m.\u001b[39;49m_init_engine, bind)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/engine/reflection.py:237\u001b[0m, in \u001b[0;36mInspector._construct\u001b[0;34m(cls, init, bind)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m bind\u001b[39m.\u001b[39mdialect\u001b[39m.\u001b[39minspector  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m)\n\u001b[0;32m--> 237\u001b[0m init(\u001b[39mself\u001b[39;49m, bind)\n\u001b[1;32m    238\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/engine/reflection.py:248\u001b[0m, in \u001b[0;36mInspector._init_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_init_engine\u001b[39m(\u001b[39mself\u001b[39m, engine: Engine) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine \u001b[39m=\u001b[39m engine\n\u001b[0;32m--> 248\u001b[0m     engine\u001b[39m.\u001b[39;49mconnect()\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    249\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_op_context_requires_connect \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdialect \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39mdialect\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/engine/base.py:3251\u001b[0m, in \u001b[0;36mEngine.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Connection:\n\u001b[1;32m   3229\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[1;32m   3230\u001b[0m \n\u001b[1;32m   3231\u001b[0m \u001b[39m    The :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3248\u001b[0m \n\u001b[1;32m   3249\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection_cls(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/engine/base.py:147\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbapi_connection \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39mraw_connection()\n\u001b[1;32m    146\u001b[0m     \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mloaded_dbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 147\u001b[0m         Connection\u001b[39m.\u001b[39;49m_handle_dbapi_exception_noconnection(\n\u001b[1;32m    148\u001b[0m             err, dialect, engine\n\u001b[1;32m    149\u001b[0m         )\n\u001b[1;32m    150\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/engine/base.py:2413\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception_noconnection\u001b[0;34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001b[0m\n\u001b[1;32m   2411\u001b[0m \u001b[39melif\u001b[39;00m should_wrap:\n\u001b[1;32m   2412\u001b[0m     \u001b[39massert\u001b[39;00m sqlalchemy_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2413\u001b[0m     \u001b[39mraise\u001b[39;00m sqlalchemy_exception\u001b[39m.\u001b[39mwith_traceback(exc_info[\u001b[39m2\u001b[39m]) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   2414\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2415\u001b[0m     \u001b[39massert\u001b[39;00m exc_info[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/engine/base.py:145\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbapi_connection \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mraw_connection()\n\u001b[1;32m    146\u001b[0m     \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mloaded_dbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    147\u001b[0m         Connection\u001b[39m.\u001b[39m_handle_dbapi_exception_noconnection(\n\u001b[1;32m    148\u001b[0m             err, dialect, engine\n\u001b[1;32m    149\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/engine/base.py:3275\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3253\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraw_connection\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PoolProxiedConnection:\n\u001b[1;32m   3254\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[1;32m   3255\u001b[0m \n\u001b[1;32m   3256\u001b[0m \u001b[39m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3273\u001b[0m \n\u001b[1;32m   3274\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool\u001b[39m.\u001b[39;49mconnect()\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/base.py:455\u001b[0m, in \u001b[0;36mPool.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PoolProxiedConnection:\n\u001b[1;32m    448\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \n\u001b[1;32m    450\u001b[0m \u001b[39m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m \n\u001b[1;32m    454\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m _ConnectionFairy\u001b[39m.\u001b[39;49m_checkout(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/base.py:1271\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[0;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_checkout\u001b[39m(\n\u001b[1;32m   1264\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     fairy: Optional[_ConnectionFairy] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1268\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m _ConnectionFairy:\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fairy:\n\u001b[0;32m-> 1271\u001b[0m         fairy \u001b[39m=\u001b[39m _ConnectionRecord\u001b[39m.\u001b[39;49mcheckout(pool)\n\u001b[1;32m   1273\u001b[0m         \u001b[39mif\u001b[39;00m threadconns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1274\u001b[0m             threadconns\u001b[39m.\u001b[39mcurrent \u001b[39m=\u001b[39m weakref\u001b[39m.\u001b[39mref(fairy)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/base.py:719\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[0;34m(cls, pool)\u001b[0m\n\u001b[1;32m    717\u001b[0m     rec \u001b[39m=\u001b[39m cast(_ConnectionRecord, pool\u001b[39m.\u001b[39m_do_get())\n\u001b[1;32m    718\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 719\u001b[0m     rec \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_do_get()\n\u001b[1;32m    721\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     dbapi_connection \u001b[39m=\u001b[39m rec\u001b[39m.\u001b[39mget_connection()\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/impl.py:168\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection()\n\u001b[1;32m    167\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[1;32m    169\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dec_overflow()\n\u001b[1;32m    170\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:147\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[39massert\u001b[39;00m exc_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    148\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/impl.py:166\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inc_overflow():\n\u001b[1;32m    165\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection()\n\u001b[1;32m    167\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m         \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/base.py:396\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_connection\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ConnectionPoolEntry:\n\u001b[1;32m    394\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m     \u001b[39mreturn\u001b[39;00m _ConnectionRecord(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/base.py:681\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[0;34m(self, pool, connect)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__pool \u001b[39m=\u001b[39m pool\n\u001b[1;32m    680\u001b[0m \u001b[39mif\u001b[39;00m connect:\n\u001b[0;32m--> 681\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__connect()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalize_callback \u001b[39m=\u001b[39m deque()\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/base.py:905\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfresh \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[1;32m    906\u001b[0m         pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mError on connect(): \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m    907\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m     \u001b[39m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     \u001b[39m# the engine, so this will usually not be set\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:147\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[39massert\u001b[39;00m exc_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    148\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/pool/base.py:901\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    900\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarttime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbapi_connection \u001b[39m=\u001b[39m connection \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_invoke_creator(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    902\u001b[0m     pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreated new connection \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, connection)\n\u001b[1;32m    903\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfresh \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/engine/create.py:636\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[0;34m(connection_record)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m             \u001b[39mreturn\u001b[39;00m connection\n\u001b[0;32m--> 636\u001b[0m \u001b[39mreturn\u001b[39;00m dialect\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/sqlalchemy/engine/default.py:580\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[0;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams):\n\u001b[1;32m    579\u001b[0m     \u001b[39m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloaded_dbapi\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[0;32m~/miniconda/envs/tf/lib/python3.10/site-packages/psycopg2/__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     kwasync[\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    121\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 122\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39;49mconnection_factory, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwasync)\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     conn\u001b[39m.\u001b[39mcursor_factory \u001b[39m=\u001b[39m cursor_factory\n",
      "\u001b[0;31mOperationalError\u001b[0m: (psycopg2.OperationalError) connection to server at \"finalgroup2.cjwyb8n4feqd.us-east-2.rds.amazonaws.com\" (18.189.90.220), port 5432 failed: FATAL:  database \"covid_and_weather_counties\" does not exist\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql_table('tablename',engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "### !!! This csv import to be replaced by import from DB instance !!!\n",
    "### !!! For model design only !!!\n",
    "#####################################################################\n",
    "\n",
    "# Read CSV data and verify\n",
    "sparse_covid_df = pd.read_csv('../data/covid/sparse_county_covid.csv',index_col=0)\n",
    "dense_covid_df = pd.read_csv('../data/covid/dense_county_covid.csv',index_col=0)\n",
    "weather_df = pd.read_csv('../data/weather/county_weather.csv',index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add population density and total population values\n",
    "density = {\n",
    "    'baltimore': 6866,\n",
    "    'essex': 6168,\n",
    "    'cook': 5301,\n",
    "    'union': 5150,\n",
    "    'norfolk': 5026,\n",
    "    'nassau': 4954,\n",
    "    'harrisonburg':4765,\n",
    "    'fairfax':2454,\n",
    "    'camden':2289,\n",
    "    'harris':2700,\n",
    "    'franklin':2186,\n",
    "    'marion':2466,\n",
    "    'dekalb':2482,\n",
    "    'duval':1305,\n",
    "    'wake':1377,\n",
    "    'bexar':1620\n",
    "}\n",
    "\n",
    "population = {\n",
    "    'baltimore': 621342,\n",
    "    'essex': 755618,\n",
    "    'cook': 5231351,\n",
    "    'union': 543976,\n",
    "    'norfolk': 245782,\n",
    "    'nassau': 74629,\n",
    "    'harrisonburg':50981,\n",
    "    'fairfax':1118602,\n",
    "    'camden':513539,\n",
    "    'harris':4253700,\n",
    "    'franklin':1195537,\n",
    "    'marion':918977,\n",
    "    'dekalb':707089,\n",
    "    'duval':879602,\n",
    "    'wake':952151,\n",
    "    'bexar':1785704\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pop_density and population columns to covid data\n",
    "sparse_covid_df['population'] = pd.Series(dtype=int)\n",
    "sparse_covid_df['pop_density'] = pd.Series(dtype=int)\n",
    "\n",
    "dense_covid_df['population'] = pd.Series(dtype=int)\n",
    "dense_covid_df['pop_density'] = pd.Series(dtype=int)\n",
    "\n",
    "# Set population and density columns for sparse counties\n",
    "for i in range(len(sparse_covid_df.index)):\n",
    "    county = sparse_covid_df.iloc[i,1]\n",
    "\n",
    "    sparse_covid_df.iloc[i,7] = population[county]\n",
    "    sparse_covid_df.iloc[i,8] = density[county]\n",
    "\n",
    "# Set population and density columns for dense counties\n",
    "for i in range(len(dense_covid_df.index)):\n",
    "    county = dense_covid_df.iloc[i,1]\n",
    "\n",
    "    dense_covid_df.iloc[i,7] = population[county]\n",
    "    dense_covid_df.iloc[i,8] = density[county]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add past_delta14 columns to sparse and dense counties\n",
    "sparse_covid_df['past_delta14'] = pd.Series(dtype=int)\n",
    "dense_covid_df['past_delta14'] = pd.Series(dtype=int)\n",
    "\n",
    "# Sparse county past_delta_14\n",
    "for i in range(14,len(sparse_covid_df.index)):\n",
    "\n",
    "    # Set past_delta_14\n",
    "    past_cases = sparse_covid_df.iloc[i-14,3]\n",
    "    present_cases = sparse_covid_df.iloc[i,3]\n",
    "    delta14 = present_cases - past_cases\n",
    "\n",
    "    sparse_covid_df.iloc[i,9] = delta14\n",
    "\n",
    "# Dense county past delta14\n",
    "for i in range(14,len(dense_covid_df.index)):\n",
    "\n",
    "    # Set past_delta_14\n",
    "    past_cases = dense_covid_df.iloc[i-14,3]\n",
    "    present_cases = dense_covid_df.iloc[i,3]\n",
    "    delta14 = present_cases - past_cases\n",
    "\n",
    "    dense_covid_df.iloc[i,9] = delta14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort weather data according to the sparse or dense county sets\n",
    "sparse_counties = sparse_covid_df['county'].unique()\n",
    "dense_counties = dense_covid_df['county'].unique()\n",
    "\n",
    "# Sort weather for each set\n",
    "sparse_weather_df = weather_df[weather_df['county'].isin(sparse_counties)]\n",
    "dense_weather_df = weather_df[weather_df['county'].isin(dense_counties)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge weather data on sparse and dense covid data\n",
    "sparse_weather_covid_df = sparse_covid_df.merge(sparse_weather_df,on=['date','county'])\n",
    "dense_weather_covid_df = dense_covid_df.merge(dense_weather_df,on=['date','county'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Sparse Data\n",
    "sparse_weather_covid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Dense Data\n",
    "dense_weather_covid_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Feature categories\n",
    "weather_features = ['temp_mean(C)','precip_sum(mm)','wind_max(km/h)','min_humidity(%)','max_humidity(%)','mean_humidity(%)']\n",
    "county_features = ['population','pop_density']\n",
    "covid_features = ['new_cases','past_delta14']\n",
    "target = ['future_delta14']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Data Model\n",
    "### All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Testing Sets\n",
    "# Drop rows with null data\n",
    "sparse = sparse_weather_covid_df.dropna()\n",
    "\n",
    "# Use All Features\n",
    "X = sparse[weather_features + covid_features + county_features]\n",
    "y = sparse[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Use linear regression model\n",
    "sparse_model = LinearRegression()\n",
    "\n",
    "# Sparse 14-day new case prediction model\n",
    "sparse_model.fit(X_train,y_train)\n",
    "training_score = sparse_model.score(X_train,y_train)\n",
    "testing_score = sparse_model.score(X_test,y_test)\n",
    "\n",
    "# View performance, coefficients and y-intercept\n",
    "print(\"------- Dense 14-day New Case Prediction --------\")\n",
    "print()\n",
    "print(\"---------------- All Features -------------------\")\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")\n",
    "print()\n",
    "print('----------------- Coefficients ------------------')\n",
    "for i in range(0,len(X.columns)):\n",
    "    print(f\"{sparse_model.feature_names_in_[i]} : {sparse_model.coef_[0][i]}\")\n",
    "print(f\"y-intercept : {sparse_model.intercept_[0]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Weather Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Testing Sets\n",
    "# Drop rows with null data\n",
    "sparse = sparse_weather_covid_df.dropna()\n",
    "\n",
    "# Use Only Weather Features\n",
    "X = sparse[weather_features]\n",
    "y = sparse[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Use linear regression model\n",
    "sparse_model = LinearRegression()\n",
    "\n",
    "# Sparse 14-day new case \n",
    "sparse_model.fit(X_train,y_train)\n",
    "training_score = sparse_model.score(X_train,y_train)\n",
    "testing_score = sparse_model.score(X_test,y_test)\n",
    "\n",
    "print(\"------- Sparse 14-day New Case Prediction -------\")\n",
    "print()\n",
    "print(\"-------------- Weather Features -----------------\")\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")\n",
    "print()\n",
    "print('----------------- Coefficients ------------------')\n",
    "for i in range(0,len(X.columns)):\n",
    "    print(f\"{sparse_model.feature_names_in_[i]} : {sparse_model.coef_[0][i]}\")\n",
    "print(f\"y-intercept : {sparse_model.intercept_[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Covid Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Testing Sets\n",
    "# Drop rows with null data\n",
    "sparse = sparse_weather_covid_df.dropna()\n",
    "\n",
    "# Use Only Covid Features\n",
    "X = sparse[covid_features]\n",
    "y = sparse[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Use linear regression model\n",
    "sparse_model = LinearRegression()\n",
    "\n",
    "# Sparse 14-day new case prediction model\n",
    "sparse_model.fit(X_train,y_train)\n",
    "training_score = sparse_model.score(X_train,y_train)\n",
    "testing_score = sparse_model.score(X_test,y_test)\n",
    "\n",
    "print(\"------- Sparse 14-day New Case Prediction -------\")\n",
    "print()\n",
    "print(\"---------------- Covid Features -----------------\")\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")\n",
    "print()\n",
    "print('----------------- Coefficients ------------------')\n",
    "for i in range(0,len(X.columns)):\n",
    "    print(f\"{sparse_model.feature_names_in_[i]} : {sparse_model.coef_[0][i]}\")\n",
    "print(f\"y-intercept : {sparse_model.intercept_[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only County Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Testing Sets\n",
    "# Drop rows with null data\n",
    "sparse = sparse_weather_covid_df.dropna()\n",
    "\n",
    "# Use County Features\n",
    "X = sparse[county_features]\n",
    "y = sparse[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Use linear regression model\n",
    "sparse_model = LinearRegression()\n",
    "\n",
    "# Sparse 14-day new case prediction model\n",
    "sparse_model.fit(X_train,y_train)\n",
    "training_score = sparse_model.score(X_train,y_train)\n",
    "testing_score = sparse_model.score(X_test,y_test)\n",
    "\n",
    "print(\"------- Sparse 14-day New Case Prediction -------\")\n",
    "print()\n",
    "print(\"--------------- County Features -----------------\")\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")\n",
    "print()\n",
    "print('----------------- Coefficients ------------------')\n",
    "for i in range(0,len(X.columns)):\n",
    "    print(f\"{sparse_model.feature_names_in_[i]} : {sparse_model.coef_[0][i]}\")\n",
    "print(f\"y-intercept : {sparse_model.intercept_[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only County / Weather Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Testing Sets\n",
    "# Drop rows with null data\n",
    "sparse = sparse_weather_covid_df.dropna()\n",
    "\n",
    "# Use Only County / Weather Features\n",
    "X = sparse[weather_features + county_features]\n",
    "y = sparse[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Use linear regression model\n",
    "sparse_model = LinearRegression()\n",
    "\n",
    "# Sparse 14-day new case prediction model\n",
    "sparse_model.fit(X_train,y_train)\n",
    "training_score = sparse_model.score(X_train,y_train)\n",
    "testing_score = sparse_model.score(X_test,y_test)\n",
    "\n",
    "print(\"------- Sparse 14-day New Case Prediction -------\")\n",
    "print()\n",
    "print(\"---------- County / Weather Features ------------\")\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")\n",
    "print()\n",
    "print('----------------- Coefficients ------------------')\n",
    "for i in range(0,len(X.columns)):\n",
    "    print(f\"{sparse_model.feature_names_in_[i]} : {sparse_model.coef_[0][i]}\")\n",
    "print(f\"y-intercept : {sparse_model.intercept_[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Covid / Weather Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Testing Sets\n",
    "# Drop rows with null data\n",
    "sparse = sparse_weather_covid_df.dropna()\n",
    "\n",
    "# Use Only Covid / Weather Features\n",
    "X = sparse[weather_features + covid_features]\n",
    "y = sparse[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Use linear regression model\n",
    "sparse_model = LinearRegression()\n",
    "\n",
    "# Sparse 14-day new case prediction model\n",
    "sparse_model.fit(X_train,y_train)\n",
    "training_score = sparse_model.score(X_train,y_train)\n",
    "testing_score = sparse_model.score(X_test,y_test)\n",
    "\n",
    "print(\"------- Sparse 14-day New Case Prediction -------\")\n",
    "print()\n",
    "print(\"---------- Covid / Weather Features -------------\")\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")\n",
    "print()\n",
    "print('----------------- Coefficients ------------------')\n",
    "for i in range(0,len(X.columns)):\n",
    "    print(f\"{sparse_model.feature_names_in_[i]} : {sparse_model.coef_[0][i]}\")\n",
    "print(f\"y-intercept : {sparse_model.intercept_[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only County / Covid Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Testing Sets\n",
    "# Drop rows with null data\n",
    "sparse = sparse_weather_covid_df.dropna()\n",
    "\n",
    "# Use Only County / Covid Features\n",
    "X = sparse[covid_features + county_features]\n",
    "y = sparse[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Use linear regression model\n",
    "sparse_model = LinearRegression()\n",
    "\n",
    "# Sparse 14-day new case \n",
    "sparse_model.fit(X_train,y_train)\n",
    "training_score = sparse_model.score(X_train,y_train)\n",
    "testing_score = sparse_model.score(X_test,y_test)\n",
    "\n",
    "print(\"------- Sparse 14-day New Case Prediction -------\")\n",
    "print()\n",
    "print(\"----------- County / Covid Features -------------\")\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")\n",
    "print()\n",
    "print('----------------- Coefficients ------------------')\n",
    "for i in range(0,len(X.columns)):\n",
    "    print(f\"{sparse_model.feature_names_in_[i]} : {sparse_model.coef_[0][i]}\")\n",
    "print(f\"y-intercept : {sparse_model.intercept_[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Data Model\n",
    "### All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Testing Sets\n",
    "# Drop rows with null data\n",
    "dense = dense_weather_covid_df.dropna()\n",
    "\n",
    "# Use All features\n",
    "X = dense[weather_features + covid_features + county_features]\n",
    "y = dense[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Use linear regression model\n",
    "dense_model = LinearRegression()\n",
    "\n",
    "# Dense 14-day new case prediction model\n",
    "dense_model.fit(X_train,y_train)\n",
    "training_score = dense_model.score(X_train,y_train)\n",
    "testing_score = dense_model.score(X_test,y_test)\n",
    "\n",
    "# View performance, coefficients and y-intercept\n",
    "print(\"------- Dense 14-day New Case Prediction --------\")\n",
    "print(\"---------------- All Features -------------------\")\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")\n",
    "print()\n",
    "print('----------------- Coefficients ------------------')\n",
    "for i in range(0,len(X.columns)):\n",
    "    print(f\"{dense_model.feature_names_in_[i]} : {dense_model.coef_[0][i]}\")\n",
    "print(f\"y-intercept : {dense_model.intercept_[0]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Weather Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Testing Sets\n",
    "# Drop rows with null data\n",
    "dense = dense_weather_covid_df.dropna()\n",
    "\n",
    "# Use Only Weather Features\n",
    "X = dense[weather_features]\n",
    "y = dense[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Use linear regression model\n",
    "dense_model = LinearRegression()\n",
    "\n",
    "# Dense 14-day new case prediction model\n",
    "dense_model.fit(X_train,y_train)\n",
    "training_score = dense_model.score(X_train,y_train)\n",
    "testing_score = dense_model.score(X_test,y_test)\n",
    "\n",
    "# View performance, coefficients and y-intercept\n",
    "print(\"------- Dense 14-day New Case Prediction --------\")\n",
    "print()\n",
    "print(\"-------------- Weather Features -----------------\")\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")\n",
    "print()\n",
    "print('----------------- Coefficients ------------------')\n",
    "for i in range(0,len(X.columns)):\n",
    "    print(f\"{dense_model.feature_names_in_[i]} : {dense_model.coef_[0][i]}\")\n",
    "print(f\"y-intercept : {dense_model.intercept_[0]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Covid Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Testing Sets\n",
    "# Drop rows with null data\n",
    "dense = dense_weather_covid_df.dropna()\n",
    "\n",
    "# Use Only Covid Features\n",
    "X = dense[covid_features]\n",
    "y = dense[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Use linear regression model\n",
    "dense_model = LinearRegression()\n",
    "\n",
    "# Dense 14-day new case prediction model\n",
    "dense_model.fit(X_train,y_train)\n",
    "training_score = dense_model.score(X_train,y_train)\n",
    "testing_score = dense_model.score(X_test,y_test)\n",
    "\n",
    "# View performance, coefficients and y-intercept\n",
    "print(\"------- Dense 14-day New Case Prediction --------\")\n",
    "print()\n",
    "print(\"---------------- Covid Features -----------------\")\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")\n",
    "print()\n",
    "print('----------------- Coefficients ------------------')\n",
    "for i in range(0,len(X.columns)):\n",
    "    print(f\"{dense_model.feature_names_in_[i]} : {dense_model.coef_[0][i]}\")\n",
    "print(f\"y-intercept : {dense_model.intercept_[0]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only County Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Testing Sets\n",
    "# Drop rows with null data\n",
    "dense = dense_weather_covid_df.dropna()\n",
    "\n",
    "# Use Only County Features\n",
    "X = dense[county_features]\n",
    "y = dense[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Use linear regression model\n",
    "dense_model = LinearRegression()\n",
    "\n",
    "# Dense 14-day new case prediction model\n",
    "dense_model.fit(X_train,y_train)\n",
    "training_score = dense_model.score(X_train,y_train)\n",
    "testing_score = dense_model.score(X_test,y_test)\n",
    "\n",
    "# View performance, coefficients and y-intercept\n",
    "print(\"------- Dense 14-day New Case Prediction --------\")\n",
    "print()\n",
    "print(\"--------------- County Features -----------------\")\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")\n",
    "print()\n",
    "print('----------------- Coefficients ------------------')\n",
    "for i in range(0,len(X.columns)):\n",
    "    print(f\"{dense_model.feature_names_in_[i]} : {dense_model.coef_[0][i]}\")\n",
    "print(f\"y-intercept : {dense_model.intercept_[0]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only County / Weather Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Testing Sets\n",
    "# Drop rows with null data\n",
    "dense = dense_weather_covid_df.dropna()\n",
    "\n",
    "# Use Only County / Weather Features\n",
    "X = dense[weather_features + county_features]\n",
    "y = dense[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Use linear regression model\n",
    "dense_model = LinearRegression()\n",
    "\n",
    "# Dense 14-day new case prediction model\n",
    "dense_model.fit(X_train,y_train)\n",
    "training_score = dense_model.score(X_train,y_train)\n",
    "testing_score = dense_model.score(X_test,y_test)\n",
    "\n",
    "# View performance, coefficients and y-intercept\n",
    "print(\"------- Dense 14-day New Case Prediction --------\")\n",
    "print()\n",
    "print(\"---------- County / Weather Features ------------\")\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")\n",
    "print()\n",
    "print('----------------- Coefficients ------------------')\n",
    "for i in range(0,len(X.columns)):\n",
    "    print(f\"{dense_model.feature_names_in_[i]} : {dense_model.coef_[0][i]}\")\n",
    "print(f\"y-intercept : {dense_model.intercept_[0]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Covid / Weather Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Testing Sets\n",
    "# Drop rows with null data\n",
    "dense = dense_weather_covid_df.dropna()\n",
    "\n",
    "# Use Only Covid / Weather Features\n",
    "X = dense[weather_features + covid_features]\n",
    "y = dense[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Use linear regression model\n",
    "dense_model = LinearRegression()\n",
    "\n",
    "# Dense 14-day new case prediction model\n",
    "dense_model.fit(X_train,y_train)\n",
    "training_score = dense_model.score(X_train,y_train)\n",
    "testing_score = dense_model.score(X_test,y_test)\n",
    "\n",
    "# View performance, coefficients and y-intercept\n",
    "print(\"------- Dense 14-day New Case Prediction --------\")\n",
    "print()\n",
    "print(\"---------- Covid / Weather Features -------------\")\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")\n",
    "print()\n",
    "print('----------------- Coefficients ------------------')\n",
    "for i in range(0,len(X.columns)):\n",
    "    print(f\"{dense_model.feature_names_in_[i]} : {dense_model.coef_[0][i]}\")\n",
    "print(f\"y-intercept : {dense_model.intercept_[0]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only County / Covid Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into Training and Testing Sets\n",
    "# Drop rows with null data\n",
    "dense = dense_weather_covid_df.dropna()\n",
    "\n",
    "# Use Only County / Covid Features\n",
    "X = dense[covid_features + county_features]\n",
    "y = dense[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "# Use linear regression model\n",
    "dense_model = LinearRegression()\n",
    "\n",
    "# Dense 14-day new case prediction model\n",
    "dense_model.fit(X_train,y_train)\n",
    "training_score = dense_model.score(X_train,y_train)\n",
    "testing_score = dense_model.score(X_test,y_test)\n",
    "\n",
    "# View performance, coefficients and y-intercept\n",
    "print(\"------- Dense 14-day New Case Prediction --------\")\n",
    "print()\n",
    "print(\"----------- County / Covid Features -------------\")\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")\n",
    "print()\n",
    "print('----------------- Coefficients ------------------')\n",
    "for i in range(0,len(X.columns)):\n",
    "    print(f\"{dense_model.feature_names_in_[i]} : {dense_model.coef_[0][i]}\")\n",
    "print(f\"y-intercept : {dense_model.intercept_[0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
